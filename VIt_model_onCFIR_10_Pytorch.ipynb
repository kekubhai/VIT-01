{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNplbiwAGzXGJ5Es4/g6ZyI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kekubhai/VIT-01/blob/main/VIt_model_onCFIR_10_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4aSzbt4Cm_xK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UvYNrPVvnChd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ir_nYuPAmZSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Qe8lOAX_lDw8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets,transforms\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YWfFwTrZpPZS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up device agnostic code\n",
        "device='cuda' if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxBUFwEhnHG1",
        "outputId": "5f43b4be-066d-4fe9-bfec-608d62631a1a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torchvision.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "poXECwqpnDhg",
        "outputId": "1958c62d-cde0-4bb2-af4e-253ea2e34f3b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.21.0+cu124'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3xXsHA6nbRz",
        "outputId": "1d82c526-0fea-4737-bc6d-e184afe4424b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the seed\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "ulsryxW1niLV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Q1KxiZaFs0M6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#setting the hyperparameters\n",
        "BATCH_SIZE=128\n",
        "EPOCHS=10\n",
        "LEARNING_RATE=3e-4\n",
        "PATCH_SIZE=4\n",
        "NUM_CLASSES=10\n",
        "IMAGE_SIZE=32\n",
        "CHANNELS=3\n",
        "EMBED_DIM=256\n",
        "NUM_HEADS=8\n",
        "DEPTH=6\n",
        "MLP_DIM=512\n",
        "DROP_RATE=0.1\n"
      ],
      "metadata": {
        "id": "S6flSgFprFT_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the transformers\n"
      ],
      "metadata": {
        "id": "OYSQ__nFs1Cw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5),(0.6))\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "o8LJGj0bsyCI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting a dataset\n",
        "train_dataset=datasets.CIFAR10(root='./data',train=True,download=True,transform=transform)\n",
        "test_dataset=datasets.CIFAR10(root='./data',train=False,download=True,transform=transform)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHx5z0NJtXUy",
        "outputId": "12684b7c-3756-4e23-bc07-cb8d82d6e517"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 62.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rf026l55tjOi",
        "outputId": "5574c70a-f60c-4317-e35f-b9c981df0bfc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 50000\n",
              "    Root location: ./data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "               Normalize(mean=0.5, std=0.6)\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuStTOFOvbEn",
        "outputId": "f3147ca3-27e5-4706-d3cd-01c768e79e21"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## converting datasets into data loaders\n",
        "train_loader=DataLoader(dataset=train_dataset,\n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        shuffle=False)\n",
        "test_loader=DataLoader(dataset=test_dataset,\n",
        "                       batch_size=BATCH_SIZE,\n",
        "                       shuffle=False)\n"
      ],
      "metadata": {
        "id": "4NHVhEMmwF0F"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's check out what we've created\n",
        "print(f\"DataLoader: {train_loader, test_loader}\")\n",
        "print(f\"Length of train loader : {len(train_loader) } batch\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kE-TET9zUSh",
        "outputId": "f8b2f16f-6bd6-41dc-c2f3-514875a4e9ac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoader: (<torch.utils.data.dataloader.DataLoader object at 0x79ff95253950>, <torch.utils.data.dataloader.DataLoader object at 0x79ff952531d0>)\n",
            "Length of train loader : 391 batch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dqk5FaYgmhpQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#building the vision transformer from scratch\n",
        "PATCH_SIZE\n"
      ],
      "metadata": {
        "id": "hWapJj910vf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4fb323d-07ba-4617-ce4f-636f615449b2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "       def __init__(self,\n",
        "                    img_size,\n",
        "                    patch_size,\n",
        "                    in_channels,\n",
        "                    embed_dim):\n",
        "         super().__init__()\n",
        "         self.patch_size=patch_size\n",
        "         self.proj=nn.Conv2(in_channels=in_channels,\n",
        "                            out_channels=embed_dim,\n",
        "                            kernel_size=patch_size,\n",
        "                            stride=patch_size)\n",
        "         num_patches=(img_size//patch_size)**2\n",
        "         self.cls_token=nn.Parameter(torch.randn(1,1,embed_dim))\n",
        "         self.pos_embedding=nn.Parameter(torch.randn(1,1+num_patches,embed_dim))\n",
        "       def forward(self,x:torch.Tensor):\n",
        "         B=x.size(0)\n",
        "         x=self.proj(x)\n",
        "         x=x.flatten(2)\n",
        "         x=x.transpose(1,2)\n",
        "         cls_token=self.cls_token(B,-1,-1)\n",
        "         x=torch.cat((cls_token,x),dim=1)\n",
        "         x=x+self.pos_embedding\n",
        "         return x"
      ],
      "metadata": {
        "id": "IHRO8Ey2o8jM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self,embed_dim, num_heads,mlp_dim,drop_rate):\n",
        "    super().__init__()\n",
        "    self.norm1=nn.LayerNorm(embed_dim)\n",
        "    self.attn=nn.MultiheadAttention(embed_dim,num_heads,dropout=drop_rate,batch_first=True)\n",
        "    self.norm2=nn.LayerNorm(embed_dim)\n",
        "    self.mlp=MLP(embed_dim,mlp_dim,drop_rate)\n",
        "  def forward(self,x):\n",
        "    x=x+self.attn(self.norm1(x),self.norm1(x),self.norm1(x))[0]\n",
        "    x=x+self.mlp(self.norm2(x))\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "KLEcDg4cuLuU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating our vision transformer class\n",
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self,\n",
        "               img_size,\n",
        "               patch_size,\n",
        "               in_channels,\n",
        "               num_classes,\n",
        "               embed_dim,\n",
        "               num_heads,\n",
        "               depth,\n",
        "               mlp_dim,\n",
        "               drop_rate):\n",
        "    super().__init__()\n",
        "    self.patch_embed=PatchEmbedding(img_size,patch_size,in_channels,embed_dim)\n",
        "    self.encoder=nn.Sequential([TransformerEncoderLayer(embed_dim,num_heads,mlp_dim,drop_rate) for _ in range(depth)])\n",
        "    self.norm=nn.LayerNorm(embed_dim)\n",
        "    self.head=nn.Linear(embed_dim,num_classes)\n",
        "    self.norm=nn.LayerNorm(embed_dim)\n",
        "    self.head=nn.Linear(embed_dim,num_classes)\n",
        "\n",
        "    def forward (self,x):\n",
        "      x=self.patch_embed(x)\n",
        "      x=self.encoder(x)\n",
        "      x=self.norm(x)\n",
        "      cls_token=x[:, 0]\n",
        "      return self.head(cls_token)\n",
        ""
      ],
      "metadata": {
        "id": "O8tFcURgveXR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ligJNtGGeaMe",
        "outputId": "eac4b1a3-e997-4ea8-fdcd-62c79f331d17"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Instandiate Model"
      ],
      "metadata": {
        "id": "LEwmy0U_ebEt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}